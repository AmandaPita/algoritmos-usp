# -*- coding: utf-8 -*-
"""Segmentação em imagens de animais domésticos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jWgg80XhRC2jr5O5-B70a3nSTwpToA5U

# Segmentação em imagens de animais domésticos: comparativo entre a ferramenta ImageJ e redes neurais convolucionais utilizando arquitetura U-Net

**Alunas:** Alessandra Ramires Souza Santos e Melissa Augusto Ribeiro<br>
**Disciplina:** 5953031 - Processamento de Imagens Médicas<br>
**Professor:** Luiz Otavio Murta Junior<br>

## Passo 0: Importação de todos as bibliotecas 
Na célula abaixo, importaremos para o ambiente de desenvolvimento do Google Colab todas as bibliotecas necessárias para o projeto. Entre elas, podemos destacar a **Pillow (PIL)** para a manipulação de imagens no formato PNG, TIFF, BMP, EPS e GIF; **Numpy** para computação numérica, manipulação de vetores n-dimencionais e semelhantes; e **Tensorflow** e **Keras** para construir a arquitetura semelhante à UNet.
"""

import os
import random
import numpy as np

import PIL
from PIL import ImageOps
from IPython.display import Image, display

from tensorflow import keras
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras import layers

"""## Passo 1: Download do dataset utilizado
O comando curl - abreviação de Cliente URL - é responsável por verificar a conectividade com a url apresentada e fazer os downloads dos arquivos que compõe o dataset utilizado de um local remoto. Por sua vez, o comando tar é o responsável por realizar a descompactação desses mesmos arquivos.
"""

!curl -O https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
!curl -O https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz
!tar -xf images.tar.gz
!tar -xf annotations.tar.gz

"""### Passo 2: Ajuste nos caminhos das imagens de input e output."""

input_dir = "images/"
target_dir = "annotations/trimaps/"
img_size = (160, 160)
num_classes = 3
batch_size = 32

input_img_paths = sorted(
    [
        os.path.join(input_dir, fname)
        for fname in os.listdir(input_dir)
        if fname.endswith(".jpg")
    ]
)
target_img_paths = sorted(
    [
        os.path.join(target_dir, fname)
        for fname in os.listdir(target_dir)
        if fname.endswith(".png") and not fname.startswith(".")
    ]
)

print("Total de amostras:", len(input_img_paths))

for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):
    print(input_path, "|", target_path)

"""## Passo 3: Imagem de entrada e máscara de segmentação correspondente 

Neste passo, verificamos se as imagens de entrada e se a sua respectiva máscara de segmentação fornecida pelo dataset OxfordPets estão sendo corretamente localizadas. A imagem é exibida na tela por meio do método display.
"""

display(Image(filename=input_img_paths[50]))
img = PIL.ImageOps.autocontrast(load_img(target_img_paths[50]))
display(img)

"""## Passo 4: Utilizando a classe `Sequence` para carregar os dados em lotes e vetorizá-los
Ao treinar um modelo, geralmente não carregamos todos os dados na memória de uma vez, mas carregamos os dados na memória em lotes. As redes neurais não processam dados brutos, como arquivos de texto, arquivos de imagem JPEG codificados ou arquivos CSV. Eles processam representações vetorizadas e padronizadas.
"""

class OxfordPets(keras.utils.Sequence):
   
    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):
        self.batch_size = batch_size
        self.img_size = img_size
        self.input_img_paths = input_img_paths
        self.target_img_paths = target_img_paths

    def __len__(self):
        return len(self.target_img_paths) // self.batch_size

    def __getitem__(self, idx):
        i = idx * self.batch_size
        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]
        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]
        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype="float32")
        for j, path in enumerate(batch_input_img_paths):
            img = load_img(path, target_size=self.img_size)
            x[j] = img
        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype="uint8")
        for j, path in enumerate(batch_target_img_paths):
            img = load_img(path, target_size=self.img_size, color_mode="grayscale")
            y[j] = np.expand_dims(img, 2)
            y[j] -= 1
        return x, y

"""## Passo 5: Construindo uma arquitetura semelhante à UNet 

"""

def get_model(img_size, num_classes):
    inputs = keras.Input(shape=img_size + (3,))

    ### [Primeira parte da arquitetura: downsampling inputs] ###
    x = layers.Conv2D(32, 3, strides=2, padding="same")(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)

    previous_block_activation = x  
    
    for filters in [64, 128, 256]:
        x = layers.Activation("relu")(x)
        x = layers.SeparableConv2D(filters, 3, padding="same")(x)
        x = layers.BatchNormalization()(x)

        x = layers.Activation("relu")(x)
        x = layers.SeparableConv2D(filters, 3, padding="same")(x)
        x = layers.BatchNormalization()(x)

        x = layers.MaxPooling2D(3, strides=2, padding="same")(x)

        residual = layers.Conv2D(filters, 1, strides=2, padding="same")(
            previous_block_activation
        )
        x = layers.add([x, residual])  
        previous_block_activation = x 

    ### [Segunda parte da arquitetura: upsampling inputs] ###

    for filters in [256, 128, 64, 32]:
        x = layers.Activation("relu")(x)
        x = layers.Conv2DTranspose(filters, 3, padding="same")(x)
        x = layers.BatchNormalization()(x)

        x = layers.Activation("relu")(x)
        x = layers.Conv2DTranspose(filters, 3, padding="same")(x)
        x = layers.BatchNormalization()(x)

        x = layers.UpSampling2D(2)(x)

        residual = layers.UpSampling2D(2)(previous_block_activation)
        residual = layers.Conv2D(filters, 1, padding="same")(residual)
        x = layers.add([x, residual])  # Add back residual
        previous_block_activation = x  # Set aside next residual

    # Adiciona uma camada de classificação por pixel
    outputs = layers.Conv2D(num_classes, 3, activation="softmax", padding="same")(x)

    # Define o model
    model = keras.Model(inputs, outputs)
    return model

keras.backend.clear_session()

# Construindo o modelo
model = get_model(img_size, num_classes)
model.summary()



"""## Passo 6: Validação
Aqui, divimos nossas imagens em um conjunto para o treinamento da rede e em um conjunto para realizar a validação. Também instanciamos as sequências de dados para cada um dos conjuntos.
"""

val_samples = 1000
random.Random(1337).shuffle(input_img_paths)
random.Random(1337).shuffle(target_img_paths)
train_input_img_paths = input_img_paths[:-val_samples]
train_target_img_paths = target_img_paths[:-val_samples]
val_input_img_paths = input_img_paths[-val_samples:]
val_target_img_paths = target_img_paths[-val_samples:]

train_gen = OxfordPets(
    batch_size, img_size, train_input_img_paths, train_target_img_paths
)
val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)

"""## Passo 7: Treinando o modelo
Em seguida, configuramos o modelo para treinamento. Usamos a versão "esparsa" de categorical_crossentropy porque nossos dados de destino são inteiros. Ao fazer o treinamento,também estamos fazendo uma validação ao final de cada época.
"""

model.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy")

callbacks = [
    keras.callbacks.ModelCheckpoint("oxford_segmentation.h5", save_best_only=True)
]

epochs = 15
model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)

"""## Resultado: Previsões feitas pela rede
Como resultado final, apresentamos as previsões realizadas para 30 imagens no conjunto de validação, comparando-as com as máscaras que foram oferecidas pelo dataset. Este número pode ser alterado conforme a necessidade. No relatório final, também é possível visualizar as mesmas imagens sendo segmentadas manualmente utilizando o software *ImageJ*.

"""

val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)
val_preds = model.predict(val_gen)


def display_mask(i):
    mask = np.argmax(val_preds[i], axis=-1)
    mask = np.expand_dims(mask, axis=-1)
    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))
    display(img)

for i in range(30):
  # Exibe imagem de entrada
  display(Image(filename=val_input_img_paths[i]))
  # Máscara de segmentação correspondente
  img = PIL.ImageOps.autocontrast(load_img(val_target_img_paths[i]))
  display(img)
  # Máscara prevista por nosso modelo
  display_mask(i)
